# ============================================================================
# FILE: utils/enhanced_chunker.py
# ============================================================================
"""
Enhanced Chunker Hibrid (Fixed-size + Semantic) dengan Metadata Preservation
Setiap chunk akan memiliki metadata lengkap dan optimal untuk RAG
"""

import time
from typing import List, Dict, Any
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document
from langchain_experimental.text_splitter import SemanticChunker
from langchain_openai import OpenAIEmbeddings
import yaml


class EnhancedChunker:
    """
    Enhanced Text Chunker dengan Metadata Preservation
    Strategi hibrid: Fixed-size + Semantic
    """
    def __init__(self, config_path: str):
        self.config = self._load_config(config_path)
        self.chunking_cfg = self.config.get("chunking", {})
        self.strategy = self.chunking_cfg.get("strategy", "hybrid")
        self.splitter_fixed = self._build_fixed_size_splitter()
        self.splitter_semantic = self._build_semantic_splitter()
    
    # ------------------------------------------------------------------------
    # CONFIG
    # ------------------------------------------------------------------------
    def _load_config(self, path: str) -> Dict[str, Any]:
        with open(path, "r", encoding="utf-8") as f:
            return yaml.safe_load(f)
    
    # ------------------------------------------------------------------------
    # BUILD SPLITTERS
    # ------------------------------------------------------------------------
    def _build_fixed_size_splitter(self):
        cfg = self.chunking_cfg.get("fixed_size", {})
        chunk_size = cfg.get("chunk_size", 1000)
        chunk_overlap = cfg.get("chunk_overlap", 200)
        separators = cfg.get("separators", ["\n\n", "\n", ".", " ", ""])
        return RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            separators=separators,
            length_function=len,
        )
    
    def _build_semantic_splitter(self):
        cfg = self.chunking_cfg.get("semantic", {})
        embeddings = OpenAIEmbeddings() if cfg.get("use_openai", True) else None
        threshold_type = cfg.get("breakpoint_threshold_type", "percentile")
        threshold_value = cfg.get("breakpoint_threshold", 95)
        return SemanticChunker(
            embeddings=embeddings,
            breakpoint_threshold_type=threshold_type,
            breakpoint_threshold_amount=threshold_value,
        )
    
    # ------------------------------------------------------------------------
    # CHUNKING DENGAN METADATA
    # ------------------------------------------------------------------------
    def chunk_with_metadata(
        self, content: str, metadata: Dict[str, str], use_semantic: bool = True
    ) -> List[Document]:
        if not content or not content.strip():
            return []
        
        # STEP 1: fixed-size split
        chunks = self.splitter_fixed.split_text(content)
        
        # STEP 2: fallback semantic untuk chunk panjang
        final_chunks = []
        for chunk in chunks:
            if use_semantic and len(chunk) > self.chunking_cfg.get("semantic", {}).get("min_length", 1200):
                sub_chunks = self.splitter_semantic.split_text(chunk)
                final_chunks.extend(sub_chunks)
            else:
                final_chunks.append(chunk)
        
        # STEP 3: attach metadata
        documents: List[Document] = []
        for idx, chunk in enumerate(final_chunks):
            chunk_metadata = metadata.copy() if metadata else {}
            chunk_metadata.update({
                "chunk_index": idx,
                "total_chunks": len(final_chunks),
                "chunk_id": f"{metadata.get('source','doc')}_{idx}",
                "chunk_strategy": self.strategy,
            })
            documents.append(
                Document(
                    id=f"{metadata.get('source','doc')}_{idx}",
                    page_content=chunk,
                    metadata=chunk_metadata,
                )
            )
        return documents
    
    def chunk_multiple_documents(self, documents: List[Dict[str, any]]) -> List[Document]:
        all_chunks: List[Document] = []
        for doc in documents:
            content = doc.get("content", "")
            metadata = doc.get("metadata", {})
            use_semantic = doc.get("use_semantic", True)
            all_chunks.extend(
                self.chunk_with_metadata(content, metadata, use_semantic=use_semantic)
            )
        return all_chunks
    
    # ------------------------------------------------------------------------
    # STATISTICS
    # ------------------------------------------------------------------------
    def get_statistics(self, documents: List[Document]) -> Dict[str, Any]:
        if not documents:
            return {"total_chunks": 0}
        
        lengths = [len(doc.page_content) for doc in documents]
        sources, jenjang = {}, {}
        for doc in documents:
            src = doc.metadata.get("source", "Unknown")
            lvl = doc.metadata.get("jenjang", "Unknown")
            sources[src] = sources.get(src, 0) + 1
            jenjang[lvl] = jenjang.get(lvl, 0) + 1
        
        return {
            "strategy": self.strategy,
            "total_chunks": len(documents),
            "avg_length": int(sum(lengths)/len(lengths)),
            "min_length": min(lengths),
            "max_length": max(lengths),
            "total_chars": sum(lengths),
            "sources": sources,
            "jenjang_distribution": jenjang,
        }


# ============================================================================
# DOCUMENT PROCESSOR
# ============================================================================
class DocumentProcessor:
    def __init__(self, chunker: EnhancedChunker, metadata_extractor):
        self.chunker = chunker
        self.metadata_extractor = metadata_extractor
    
    def process_document(
        self,
        filename: str,
        content: str,
        metadata: Dict[str,str] = None,
        use_semantic: bool = True
    ) -> List[Document]:
        print(f"\nðŸ“„ Processing: {filename}")
        if metadata:
            print("   âœ… Using provided metadata")
        else:
            metadata = self.metadata_extractor.extract_full(filename, content)
            print("   ðŸ” Metadata extracted automatically")
        
        chunks = self.chunker.chunk_with_metadata(content, metadata, use_semantic=use_semantic)
        print(f"   âœ‚ï¸ Created {len(chunks)} chunks")
        return chunks
    
    def process_multiple_documents(self, documents: List[Dict[str, any]]) -> List[Document]:
        all_chunks: List[Document] = []
        for doc in documents:
            filename = doc.get("filename", "Unknown.pdf")
            content = doc.get("content", "")
            metadata = doc.get("metadata", {})
            use_semantic = doc.get("use_semantic", True)
            all_chunks.extend(
                self.process_document(filename, content, metadata, use_semantic)
            )
        stats = self.chunker.get_statistics(all_chunks)
        print(f"\nðŸ“Š Total chunks: {stats['total_chunks']}, Avg length: {stats['avg_length']} chars")
        return all_chunks
