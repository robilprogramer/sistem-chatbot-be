# ğŸ” Enhanced RAG dengan Document Aggregation

## Solusi untuk Masalah "Informasi Terpotong"

Dokumentasi ini menjelaskan solusi untuk masalah chunks yang terpotong saat retrieval, sehingga context yang diberikan ke LLM menjadi lengkap.

---

## ğŸ“Š Diagram Alur

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           INGESTION PIPELINE                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  Document          Enhanced Chunker              ChromaDB                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚ content  â”‚      â”‚ 1. Generate        â”‚       â”‚ Store dengan     â”‚        â”‚
â”‚  â”‚ metadata â”‚ â”€â”€â–¶  â”‚    document_id     â”‚  â”€â”€â–¶  â”‚ document_id      â”‚        â”‚
â”‚  â”‚          â”‚      â”‚    (KONSISTEN)     â”‚       â”‚ di metadata      â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚                    â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                    â”‚ 2. Split ke chunks â”‚                                    â”‚
â”‚                    â”‚    dengan:         â”‚       Setiap chunk punya:          â”‚
â”‚                    â”‚    - chunk_index   â”‚       âœ“ document_id (sama)         â”‚
â”‚                    â”‚    - chunk_id      â”‚       âœ“ chunk_index (urutan)       â”‚
â”‚                    â”‚    - prev/next     â”‚       âœ“ chunk_id (unik)            â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                    â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           RETRIEVAL PIPELINE                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  Query              Smart Retriever                    LLM                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚ "Berapa  â”‚      â”‚ 1. Get top-K chunks        â”‚     â”‚              â”‚      â”‚
â”‚  â”‚  biaya   â”‚ â”€â”€â–¶  â”‚                            â”‚     â”‚   Generate   â”‚      â”‚
â”‚  â”‚  SPP?"   â”‚      â”‚ 2. Extract document_ids    â”‚ â”€â”€â–¶ â”‚   Answer     â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚    (chunks mana yg relevan)â”‚     â”‚              â”‚      â”‚
â”‚                    â”‚                            â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                    â”‚ 3. Fetch ALL chunks        â”‚                            â”‚
â”‚                    â”‚    per document_id         â”‚     Context yang           â”‚
â”‚                    â”‚                            â”‚     dikirim ke LLM:        â”‚
â”‚                    â”‚ 4. Sort by chunk_index     â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚                    â”‚                            â”‚     â”‚ [Dokumen 1]  â”‚      â”‚
â”‚                    â”‚ 5. MERGE chunks            â”‚     â”‚ Chunk 0 + 1  â”‚      â”‚
â”‚                    â”‚    jadi dokumen utuh       â”‚     â”‚ + 2 + 3 ...  â”‚      â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚ (LENGKAP!)   â”‚      â”‚
â”‚                                                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”‘ Konsep Kunci

### 1. Document ID yang Konsisten

**Masalah Sebelumnya:**
- Chunks dari dokumen yang sama punya ID berbeda-beda
- Tidak bisa menggabungkan chunks saat retrieval

**Solusi:**
```python
def _generate_document_id(self, source: str, content: str) -> str:
    """
    Generate document_id berdasarkan source + content hash
    SEMUA chunks dari dokumen yang sama punya document_id SAMA
    """
    content_preview = content[:500]
    unique_string = f"{source}_{hashlib.md5(content_preview.encode()).hexdigest()[:8]}"
    return unique_string
```

### 2. Metadata Lengkap per Chunk

Setiap chunk menyimpan:
```python
chunk_metadata = {
    # Identifikasi dokumen
    "document_id": "brosur_biaya_2024_a1b2c3d4",  # SAMA untuk semua chunks
    "chunk_id": "brosur_biaya_2024_a1b2c3d4_chunk_0001",
    
    # Posisi dalam dokumen
    "chunk_index": 0,
    "total_chunks": 5,
    "is_first_chunk": True,
    "is_last_chunk": False,
    
    # Navigasi
    "prev_chunk_id": None,
    "next_chunk_id": "brosur_biaya_2024_a1b2c3d4_chunk_0002",
    
    # Metadata asli
    "source": "brosur_biaya_2024.pdf",
    "jenjang": "SD",
    "tahun": "2024/2025",
    "cabang": "Pusat"
}
```

### 3. Document Aggregation saat Retrieval

```python
def _aggregate_by_document(self, initial_docs, verbose):
    """
    KUNCI: Fetch SEMUA chunks dari dokumen yang sama
    """
    
    # 1. Group hasil retrieval by document_id
    docs_by_id = defaultdict(list)
    for doc in initial_docs:
        doc_id = doc.metadata.get('document_id')
        docs_by_id[doc_id].append(doc)
    
    # 2. Untuk setiap dokumen unik, fetch SEMUA chunks
    for doc_id in docs_by_id.keys():
        all_chunks = self._fetch_all_chunks_for_document(doc_id)
        
        # 3. Sort by chunk_index
        all_chunks.sort(key=lambda x: x.metadata.get('chunk_index', 0))
        
        # 4. Merge jadi satu dokumen
        merged = self._merge_chunks(all_chunks)
```

---

## ğŸ“ Struktur File

```
rag_solution/
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.yaml         # Konfigurasi RAG
â”œâ”€â”€ core/
â”‚   â””â”€â”€ rag_factory.py      # Factory pattern untuk components
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ enhanced_chunker.py # Chunker dengan document tracking
â”‚   â””â”€â”€ smart_retriever.py  # Retriever dengan aggregation
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ chat_router.py      # Chat endpoint
â”‚   â””â”€â”€ ingestion_router.py # Ingestion endpoint
â””â”€â”€ main.py                 # FastAPI app
```

---

## ğŸš€ Quick Start

### 1. Install Dependencies

```bash
pip install fastapi uvicorn langchain langchain-openai langchain-chroma chromadb pydantic python-dotenv pyyaml
```

### 2. Set Environment

```bash
# .env
OPENAI_API_KEY=sk-xxx
```

### 3. Ingest Document

```python
from core.rag_factory import get_chunker, get_vectorstore

# Chunk document
chunker = get_chunker()
chunks = chunker.chunk_document(
    content="Biaya SPP SD tahun 2024/2025 adalah Rp 1.500.000...",
    metadata={
        "source": "brosur_biaya_2024.pdf",
        "jenjang": "SD",
        "tahun": "2024/2025"
    }
)

# Store ke vectorstore
vectorstore = get_vectorstore()
vectorstore.add_documents(
    documents=chunks,
    ids=[c.metadata['chunk_id'] for c in chunks]
)
```

### 4. Query

```python
from core.rag_factory import get_query_chain

chain = get_query_chain()
result = chain.query("Berapa biaya SPP SD?")

print(result['answer'])
print(result['sources'])
```

---

## ğŸ”§ API Endpoints

### Chat

```bash
# Simple chat
POST /api/v1/chat/
{
    "question": "Berapa biaya SPP SD tahun 2024?",
    "verbose": false
}

# Test retrieval (tanpa LLM)
POST /api/v1/chat/test-retrieval
{
    "query": "biaya SPP",
    "top_k": 5,
    "fetch_full_document": true
}

# Debug vectorstore
GET /api/v1/chat/debug
```

### Ingestion

```bash
# Ingest single document
POST /api/v1/ingest/document
{
    "content": "...",
    "metadata": {
        "source": "brosur.pdf",
        "jenjang": "SD",
        "tahun": "2024"
    }
}

# List documents
GET /api/v1/ingest/status

# Get chunks for document
GET /api/v1/ingest/document/{document_id}/chunks

# Delete document
DELETE /api/v1/ingest/document/{document_id}
```

---

## ğŸ“Š Debug VectorStore

### Cek Struktur

```python
from core.rag_factory import inspect_vectorstore

info = inspect_vectorstore()
print(f"Total chunks: {info['total_chunks']}")
print(f"Unique documents: {info['unique_documents']}")
print(f"Document IDs: {info['document_ids']}")
print(f"Sample metadata: {info['sample_metadata']}")
```

### Cek Document Aggregation

```python
from core.rag_factory import get_retriever

retriever = get_retriever()
docs = retriever.retrieve("biaya SPP", verbose=True)

for doc in docs:
    print(f"Document ID: {doc.metadata['document_id']}")
    print(f"Merged chunks: {doc.metadata.get('merged_chunks', 1)}")
    print(f"Content length: {len(doc.page_content)}")
```

---

## âš™ï¸ Tuning Parameters

### Chunking

```yaml
chunking:
  fixed_size:
    # Dokumen informatif (brosur, FAQ)
    chunk_size: 800
    chunk_overlap: 150
    
    # Dokumen panjang (prosedur, regulasi)
    chunk_size: 1200
    chunk_overlap: 250
```

### Retrieval

```yaml
retrieval:
  # Jumlah chunks awal
  top_k: 5
  
  # Similarity minimum (0.5 = cukup relevan)
  similarity_threshold: 0.5
  
  # Max dokumen unik (3 = balance antara detail dan noise)
  max_documents: 3
  
  # PENTING: true untuk menggabungkan chunks
  fetch_full_document: true
```

---

## ğŸ” Troubleshooting

### 1. Chunks Tidak Tergabung

**Cek document_id:**
```python
results = vectorstore._collection.get(include=['metadatas'])
for meta in results['metadatas']:
    print(meta.get('document_id'))
```

**Pastikan document_id konsisten:**
- Semua chunks dari file yang sama harus punya document_id sama

### 2. Informasi Masih Terpotong

**Increase chunk_size:**
```yaml
chunking:
  fixed_size:
    chunk_size: 1500  # Lebih besar
    chunk_overlap: 300
```

**Atau increase max_documents:**
```yaml
retrieval:
  max_documents: 5  # Lebih banyak dokumen
```

### 3. Hasil Tidak Relevan

**Turunkan similarity_threshold:**
```yaml
retrieval:
  similarity_threshold: 0.4  # Lebih permisif
```

---

## ğŸ—ï¸ Integrasi dengan Kode Existing

### Ganti Chunker

```python
# SEBELUM (existing)
from langchain_text_splitters import RecursiveCharacterTextSplitter
splitter = RecursiveCharacterTextSplitter(chunk_size=1000)
chunks = splitter.split_text(content)

# SESUDAH (enhanced)
from utils.enhanced_chunker import EnhancedChunker
chunker = EnhancedChunker(config=config)
chunks = chunker.chunk_document(content, metadata)
# chunks sekarang punya document_id yang konsisten!
```

### Ganti Retriever

```python
# SEBELUM (existing)
docs = vectorstore.similarity_search(query, k=5)

# SESUDAH (smart)
from utils.smart_retriever import SmartRetriever
retriever = SmartRetriever(
    vectorstore=vectorstore,
    embedding_function=embeddings,
    fetch_full_document=True  # KUNCI!
)
docs = retriever.retrieve(query)
# docs sekarang sudah digabungkan per dokumen!
```

---

## ğŸ“ Catatan Penting

1. **document_id HARUS konsisten** - Semua chunks dari dokumen yang sama WAJIB punya document_id yang sama. Ini adalah kunci untuk aggregation.

2. **Re-ingest jika perlu** - Jika data existing tidak punya document_id yang konsisten, perlu re-ingest dengan chunker baru.

3. **Balance chunk_size** - Terlalu kecil = konteks terpotong, terlalu besar = noise. Rekomendasi: 800-1200 chars.

4. **fetch_full_document = true** - Ini yang memungkinkan penggabungan chunks. Jangan dimatikan kecuali untuk testing.

5. **Monitor context length** - Pastikan total context tidak melebihi context window LLM (biasanya 4K-128K tokens).

---

## ğŸ”„ Migration dari Kode Existing

Jika Anda sudah punya data di ChromaDB tanpa document_id yang konsisten:

```python
# Script migration
from core.rag_factory import get_vectorstore, get_chunker

vectorstore = get_vectorstore()
chunker = get_chunker()

# 1. Export existing data
collection = vectorstore._collection
all_data = collection.get(include=['documents', 'metadatas'])

# 2. Group by source (asumsi source adalah identifier)
from collections import defaultdict
docs_by_source = defaultdict(list)

for content, metadata in zip(all_data['documents'], all_data['metadatas']):
    source = metadata.get('source', 'unknown')
    docs_by_source[source].append({
        'content': content,
        'metadata': metadata
    })

# 3. Re-chunk dengan document_id baru
new_chunks = []
for source, docs in docs_by_source.items():
    # Gabungkan content dari chunks yang sama source
    full_content = "\n\n".join([d['content'] for d in docs])
    base_metadata = docs[0]['metadata']
    
    # Chunk ulang
    chunks = chunker.chunk_document(full_content, base_metadata)
    new_chunks.extend(chunks)

# 4. Clear dan re-populate
# HATI-HATI: Ini akan menghapus data lama!
collection.delete(ids=all_data['ids'])

# Add new chunks
vectorstore.add_documents(
    documents=new_chunks,
    ids=[c.metadata['chunk_id'] for c in new_chunks]
)

print(f"Migrated {len(new_chunks)} chunks")
```

---

## ğŸ“ˆ Performance Tips

### 1. Batch Embedding

```python
# Untuk dokumen banyak, gunakan batch
chunker = get_chunker()
all_chunks = []

for doc in documents:
    chunks = chunker.chunk_document(doc['content'], doc['metadata'])
    all_chunks.extend(chunks)

# Batch add (lebih efisien)
vectorstore.add_documents(
    documents=all_chunks,
    ids=[c.metadata['chunk_id'] for c in all_chunks]
)
```

### 2. Caching Query Chain

```python
# Query chain sudah singleton, jadi aman dipanggil berkali-kali
chain = get_query_chain()  # Hanya init sekali

# Subsequent calls return same instance
chain = get_query_chain()  # Tidak init ulang
```

---

## ğŸ§ª Testing

### Unit Test Chunker

```python
def test_document_id_consistency():
    chunker = EnhancedChunker(config=config)
    
    content = "Lorem ipsum dolor sit amet..." * 100
    metadata = {"source": "test.pdf"}
    
    chunks = chunker.chunk_document(content, metadata)
    
    # Semua chunks harus punya document_id yang sama
    doc_ids = set(c.metadata['document_id'] for c in chunks)
    assert len(doc_ids) == 1, "document_id tidak konsisten!"
    
    # Chunk index harus berurutan
    indices = [c.metadata['chunk_index'] for c in chunks]
    assert indices == list(range(len(chunks))), "chunk_index tidak urut!"
```

### Integration Test

```python
def test_retrieval_aggregation():
    # Ingest test document
    chunker = get_chunker()
    vectorstore = get_vectorstore()
    
    content = "Bagian 1: Biaya SPP\n" * 50 + "Bagian 2: Pendaftaran\n" * 50
    chunks = chunker.chunk_document(content, {"source": "test_doc.pdf"})
    
    vectorstore.add_documents(chunks, ids=[c.metadata['chunk_id'] for c in chunks])
    
    # Retrieve
    retriever = get_retriever()
    docs = retriever.retrieve("biaya SPP")
    
    # Harus ada aggregation
    assert len(docs) > 0
    assert docs[0].metadata.get('is_aggregated', False) == True
    assert docs[0].metadata.get('merged_chunks', 0) > 1
```

---

## ğŸ¤ Summary

**Key Improvements:**
1. âœ… Document ID konsisten untuk semua chunks dari dokumen yang sama
2. âœ… Chunk metadata lengkap dengan navigasi (prev/next)
3. âœ… Smart retrieval dengan document aggregation
4. âœ… Debug endpoints untuk troubleshooting
5. âœ… Migration support untuk data existing

**Flow Ringkas:**
```
Dokumen â†’ Chunk (dengan document_id sama) â†’ Embed â†’ Store
Query â†’ Retrieve top-K â†’ Group by document_id â†’ Fetch ALL chunks â†’ Merge â†’ LLM
```

Dengan solusi ini, informasi tidak akan terpotong karena semua chunks dari dokumen yang sama akan digabungkan sebelum dikirim ke LLM.
